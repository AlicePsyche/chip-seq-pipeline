#!/usr/bin/env python
# accessionator 0.0.1
# Generated by dx-app-wizard.
#
# Basic execution pattern: Your app will run on a single machine from
# beginning to end.
#
# See https://wiki.dnanexus.com/Developer-Portal for documentation and
# tutorials on how to modify this file.
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import os, requests, logging, re, urlparse, subprocess, requests, json, shlex, time
import dxpy

try:
    DX_FS_ROOT = os.environ['DX_FS_ROOT']
except:
    DX_FS_ROOT = ""
KEYFILE = DX_FS_ROOT + '/keypairs.json'
DEFAULT_SERVER = 'https://www.encodeproject.org'
S3_SERVER='s3://encode-files/'
logger = logging.getLogger(__name__)

FILE_OBJ_TEMPLATE = {
        #'notes': 'Biorep%d | Mapped to %s' %(biorep_n, input_shield_stage_input.get('reference_tar')),
        'lab': 'j-michael-cherry',
        'award': 'U41HG006992',
        'file_format': 'bam',
        'output_type': 'alignments',
        #'derived_from': derived_from,
        #'dataset': experiment.get('accession')}
}


def processkey(key):

    if key:
        keysf = open(KEYFILE,'r')
        keys_json_string = keysf.read()
        keysf.close()
        keys = json.loads(keys_json_string)
        logger.debug("Keys: %s" %(keys))
        key_dict = keys[key]
    else:
        key_dict = {}
    AUTHID = key_dict.get('key')
    AUTHPW = key_dict.get('secret')
    if key:
        SERVER = key_dict.get('server')
    else:
        SERVER = 'https://www.encodeproject.org/'

    if not SERVER.endswith("/"):
        SERVER += "/"

    return (AUTHID,AUTHPW,SERVER)

def encoded_get(url, AUTHID=None, AUTHPW=None):
    HEADERS = {'content-type': 'application/json'}
    if AUTHID and AUTHPW:
        response = requests.get(url, auth=(AUTHID,AUTHPW), headers=HEADERS)
    else:
        response = requests.get(url, headers=HEADERS)
    return response

def encoded_post(url, AUTHID, AUTHPW, payload):
    HEADERS = {'content-type': 'application/json'}
    response = requests.post(url, auth=(AUTHID,AUTHPW), headers=HEADERS, data=json.dumps(payload))
    return response

def s3cp(accession, key=None):

    (AUTHID,AUTHPW,SERVER) = processkey(key)

    url = SERVER + '/search/?type=file&accession=%s&format=json&frame=embedded&limit=all' %(accession)
    #get the file object
    response = encoded_get(url, AUTHID, AUTHPW)
    logger.debug(response)

    #select your file
    f_obj = response.json()['@graph'][0]
    logger.debug(f_obj)

    #make the URL that will get redirected - get it from the file object's href property
    encode_url = urlparse.urljoin(SERVER,f_obj.get('href'))
    logger.debug("URL: %s" %(encode_url))
    logger.debug("%s:%s" %(AUTHID, AUTHPW))
    #stream=True avoids actually downloading the file, but it evaluates the redirection
    r = requests.get(encode_url, auth=(AUTHID,AUTHPW), headers={'content-type': 'application/json'}, allow_redirects=True, stream=True)
    try:
        r.raise_for_status
    except:
        logger.error('%s href does not resolve' %(f_obj.get('accession')))
    logger.debug("Response: %s", (r))

    #this is the actual S3 https URL after redirection
    s3_url = r.url
    logger.debug(s3_url)

    #release the connection
    r.close()

    #split up the url into components
    o = urlparse.urlparse(s3_url)

    #pull out the filename
    filename = os.path.basename(o.path)

    #hack together the s3 cp url (with the s3 method instead of https)
    bucket_url = S3_SERVER.rstrip('/') + o.path

    #cp the file from the bucket
    subprocess.check_call(shlex.split('aws s3 cp %s . --quiet' %(bucket_url)), stderr=subprocess.STDOUT)
    subprocess.check_call(shlex.split('ls -l %s' %(filename)))

    dx_file = dxpy.upload_local_file(filename)

    return dx_file

def resolve_project(identifier, privs='r'):
    logger.debug("In resolve_project with identifier %s" %(identifier))
    project = dxpy.find_one_project(name=identifier, level='VIEW', name_mode='exact', return_handler=True, zero_ok=True)
    if project == None:
        try:
            project = dxpy.get_handler(identifier)
        except:
            logger.error('Could not find a unique project with name or id %s' %(identifier))
            raise ValueError(identifier)
    logger.debug('Project %s access level is %s' %(project.name, project.describe()['level']))
    if privs == 'w' and project.describe()['level'] == 'VIEW':
        logger.error('Output project %s is read-only' %(identifier))
        raise ValueError(identifier)
    return project

def resolve_folder(project, identifier):
    if not identifier.startswith('/'):
        identifier = '/' + identifier
    try:
        project_id = project.list_folder(identifier)
    except:
        try:
            project_id = project.new_folder(identifier, parents=True)
        except:
            logger.error("Cannot create folder %s in project %s" %(identifier, project.name))
            raise ValueError('%s:%s' %(project.name, identifier))
        else:
            logger.info("New folder %s created in project %s" %(identifier, project.name))
    return identifier


def resolve_accession(accession, key):
    logger.debug("Looking for accession %s" %(accession))
    
    if not re.match(r'''^ENCFF\d{3}[A-Z]{3}''', accession):
        logger.warning("%s is not a valid accession format" %(accession))
        return None
    
    DNANEXUS_ENCODE_SNAPSHOT = 'ENCODE-SDSC-snapshot-20140505'

    logger.debug('Looking for snapshot project %s' %(DNANEXUS_ENCODE_SNAPSHOT))
    try:
        project_handler = resolve_project(DNANEXUS_ENCODE_SNAPSHOT)
        snapshot_project = project_handler
    except:
        logger.error("Cannot find snapshot project %s" %(DNANEXUS_ENCODE_SNAPSHOT))
        snapshot_project = None

    logger.debug('Snapshot project: %s' %(snapshot_project))

    if snapshot_project:
        try:
            accession_search = accession + '*'
            logger.debug('Looking recursively for %s in %s' %(accession_search, snapshot_project.name))
            file_handler = dxpy.find_one_data_object(
                name=accession_search, name_mode='glob', more_ok=False, classname='file', recurse=True, return_handler=True,
                folder='/', project=snapshot_project.get_id())
            logger.debug('Got file handler for %s' %(file_handler.name))
            return file_handler
        except:
            logger.debug("Cannot find accession %s in project %s" %(accession, snapshot_project))

    # we're here because we couldn't find the cache or couldn't find the file in the cache, so look in AWS
    
    dx_file = s3cp(accession, key) #this returns a link to the file in the applet's project context

    if not dx_file:
        logger.warning('Cannot find %s.  Giving up.' %(accession))
        return None
    else:
        return dx_file

def resolve_file(identifier, key):
    logger.debug("resolve_file: %s" %(identifier))

    if not identifier:
        return None

    m = re.match(r'''^([\w\-\ \.]+):([\w\-\ /\.]+)''', identifier)
    if m: #fully specified with project:path
        project_identifier = m.group(1)
        file_identifier = m.group(2)
    else:
        logger.debug("Defaulting to the current project")
        project_identifier = dxpy.WORKSPACE_ID
        file_identifier = identifier    

    project = resolve_project(project_identifier)
    logger.debug("Got project %s" %(project.name))
    logger.debug("Now looking for file %s" %(file_identifier))

    m = re.match(r'''(^[\w\-\ /\.]+)/([\w\-\ \.]+)''', file_identifier)
    if m:
        folder_name = m.group(1)
        if not folder_name.startswith('/'):
            folder_name = '/' + folder_name
        file_name = m.group(2)
    else:
        folder_name = '/'
        file_name = file_identifier

    logger.debug("Looking for file %s in folder %s" %(file_name, folder_name))

    try:
        file_handler = dxpy.find_one_data_object(name=file_name, folder=folder_name, project=project.get_id(),
            more_ok=False, zero_ok=False, return_handler=True)
    except:
        logger.debug('%s not found in project %s folder %s' %(file_name, project.get_id(), folder_name))
        try: #maybe it's just  filename in the default workspace
            file_handler = dxpy.DXFile(dxid=identifier, mode='r')
        except:
            logger.debug('%s not found as a dxid' %(identifier))
            file_handler = resolve_accession(identifier, key)

    if not file_handler:
        logger.warning("Failed to resolve file identifier %s" %(identifier))
        return None
    else:
        logger.debug("Resolved file identifier %s to %s" %(identifier, file_handler.name))
        return file_handler


@dxpy.entry_point('main')
def main(folder_name, key_name, debug):

    if debug:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)

    if not folder_name.startswith('/'):
    	folder_name = '/' + folder_name

    try:
        project = dxpy.DXProject(dxpy.PROJECT_CONTEXT_ID)
        project_name = project.describe().get('name')
    except:
        print "Project not found"
        project_name = ""

 
    bams = dxpy.find_data_objects(
    	classname="file",
    	state="closed",
    	name="*.bam",
    	name_mode="glob",
    	project=dxpy.PROJECT_CONTEXT_ID,
    	folder=folder_name,
    	recurse=True,
    	return_handler=True
	)

    authid, authpw, server = processkey(key_name)
    if not subprocess.call('which md5', shell=True):
        md5_command = 'md5 -q'
    elif not subprocess.call('which md5sum', shell=True):
        md5_command = 'md5sum'
    else:
        print "Cannot find md5 or md5sum command"
        md5_command = ''

    file_mapping = []
    for bam in bams:
        bam_description = bam.describe()
        experiment_accession = re.match('\S*(ENC\S{8})',bam.folder).group(1)
        dxpy.download_dxfile(bam.get_id(),bam.name)
        md5_output = subprocess.check_output(' '.join([md5_command, bam.name]), shell=True)
        calculated_md5 = md5_output.partition(' ')[0].rstrip()
        encode_object = FILE_OBJ_TEMPLATE
        encode_object.update({
            'dataset': experiment_accession,
            'notes': '{"dx-id":"%s", "dx-createdBy":%s}' %(bam_description.get('id'), bam_description.get('createdBy')),
            'submitted_file_name': project_name + ':' + '/'.join([bam.folder,bam.name]),
            'derived_from': re.findall('(ENCFF\S{6})',bam.name),
            'file_size': bam_description.get('size'),
            'md5sum': calculated_md5
            })
        print "Experiment accession: %s" %(experiment_accession)
        print "File metadata: %s" %(encode_object)

        url = urlparse.urljoin(server,'files')
        r = encoded_post(url, authid, authpw, encode_object)
        try:
            r.raise_for_status()
            item = r.json()['@graph'][0]
            print "New accession: %s" %(item.get('accession'))
        except:
            print('POST file object failed: %s %s' % (r.status_code, r.reason))
            print(r.text)
            item = {}

        if item:
            creds = item['upload_credentials']
            env = os.environ.copy()
            env.update({
                'AWS_ACCESS_KEY_ID': creds['access_key'],
                'AWS_SECRET_ACCESS_KEY': creds['secret_key'],
                'AWS_SECURITY_TOKEN': creds['session_token'],
            })

            print("Uploading file.")
            start = time.time()
            try:
                subprocess.check_call(['aws', 's3', 'cp', bam.name, creds['upload_url']], env=env)
            except subprocess.CalledProcessError as e:
                # The aws command returns a non-zero exit code on error.
                print("Upload failed with exit code %d" % e.returncode)
                upload_returncode = e.returncode
            else:
                upload_returncode = 0
                end = time.time()
                duration = end - start
                print("Uploaded in %.2f seconds" % duration)
        else:
            upload_returncode = -1

        out_string = ','.join([experiment_accession, encode_object.get('submitted_file_name'), item.get('accession') or '', str(upload_returncode)])
        print out_string
        file_mapping.append(out_string)

    output_log_filename = time.strftime('%m%d%y%H%M') + '-accession_log.csv'
    out_fh = dxpy.upload_string('\n'.join(file_mapping), name=output_log_filename, media_type='text/csv')
    out_fh.close()

    output = {
        "file_mapping": file_mapping,
        "outfile": dxpy.dxlink(out_fh)
    }

    return output

dxpy.run()

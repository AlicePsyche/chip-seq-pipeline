#!/usr/bin/env python
# accession_histone_analysis 0.0.1
# Generated by dx-app-wizard.
#
# Basic execution pattern: Your app will run on a single machine from
# beginning to end.
#
# See https://wiki.dnanexus.com/Developer-Portal for documentation and
# tutorials on how to modify this file.
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import os, sys, subprocess, logging, dxpy, json, re, socket, getpass, urlparse, datetime, requests, time
import common
import dateutil.parser

#logging.getLogger("requests").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)
logger.addHandler(dxpy.DXLogHandler())
logger.propagate = False
# logger = logging.getLogger(__name__)
# logger.addHandler()

# class Accessionable_File(object):
# 	def __init__(self, ):

common_metadata = {
	'lab': 'encode-processing-pipeline',
	'award': 'U41HG006992',
}

def get_rep_bams(experiment, assembly, keypair, server):

	original_files = [common.encoded_get(urlparse.urljoin(server,'%s' %(uri)), keypair) for uri in experiment.get('original_files')]

	#resolve the biorep_n for each fastq
	for fastq in [f for f in original_files if f.get('file_format') == 'fastq']:
		replicate = common.encoded_get(urlparse.urljoin(server,'%s' %(fastq.get('replicate'))), keypair)
		fastq.update({'biorep_n' : replicate.get('biological_replicate_number')})
	#resolve the biorep_n's from derived_from for each bam
	for bam in [f for f in original_files if f.get('file_format') == 'bam' and f.get('assembly') == assembly]:
		biorep_ns = set()
		for derived_from_uri in bam.get('derived_from'):
			derived_from_accession = os.path.basename(derived_from_uri.strip('/')) #this assumes frame=object
			biorep_ns.add(next(f.get('biorep_n') for f in original_files if f.get('accession') == derived_from_accession))
		if len(biorep_ns) != 1:
			logger.error("%s %s expected 1 biorep_n, found %d, skipping." %(experiment_accession, bam.get('accession')))
			return
		else:
			biorep_n = biorep_ns.pop()
			bam.update({'biorep_n': biorep_n})
	#remove any bams that are older than another bam (resulting in only the most recent surviving)
	for bam in [f for f in original_files if f.get('file_format') == 'bam' and f.get('biorep_n') == biorep_n and common.after(bam.get('date_created'), f.get('date_created'))]:
		original_files.remove(bam)

	try:
		rep1_bam = next(f for f in original_files if f.get('file_format') == 'bam' and f.get('biorep_n') == 1)
	except StopIteration:
		logger.error('%s has no rep1 bam.' %(experiment.get('accession')))
		rep1_bam = None
	try:
		rep2_bam = next(f for f in original_files if f.get('file_format') == 'bam' and f.get('biorep_n') == 2)
	except StopIteration:
		logger.error('%s has no rep2 bam.' %(experiment.get('accession')))
		rep2_bam = None
	logger.debug('get_rep_bams returning %s, %s' %(rep1_bam.get('accession'),rep2_bam.get('accession')))
	return rep1_bam, rep2_bam

def get_rep_fastqs(experiment, keypair, server, reps=[1,2]):

	original_files = [common.encoded_get(urlparse.urljoin(server,'%s' %(uri)), keypair) for uri in experiment.get('original_files')]
	fastqs = [f for f in original_files if f.get('file_format') == 'fastq']

	#resolve the biorep_n for each fastq
	rep_fastqs = [
		[f for f in fastqs if common.encoded_get(urlparse.urljoin(server,'%s' %(f.get('replicate'))), keypair).get('biological_replicate_number') == n]
		for n in reps]


	# for fastq in [f for f in original_files if f.get('file_format') == 'fastq']:
	# 	replicate = common.encoded_get(urlparse.urljoin(server,'%s' %(fastq.get('replicate'))), keypair)
	# 	rep_n = replicate.get('biological_replicate_number')
	# 	if rep_n in reps:

	# 	if rep_n == 1:
	# 		rep1_fastqs.append(fastq)
	# 	elif rep_n == 2:
	# 		rep2_fastqs.append(fastq)
	# 	else:
	# 		logger.error('%s: Found fastq with biorep_n %d not in (1,2)' %(experiment['accession'], rep_n)) 
	#logger.debug('get_rep_fastqs found rep_fastqs: %s' %(rep_fastqs))
	logger.debug('get_rep_fastqs returning %s' %([[f.get('accession') for f in rep_fastqs[n]] for n,r in enumerate(reps)]))
	return rep_fastqs

def get_stage_metadata(analysis, stage_name):
	logger.debug('in get_stage_metadata with')
	logger.debug('analysis %s and stage_name %s' %(analysis['id'], stage_name))
	return next(s['execution'] for s in analysis.get('stages') if re.match(stage_name,s['execution']['name']))

def get_mapping_stages(peaks_analysis, experiment, keypair, server, reps=[1,2]):
	# Find the tagaligns actually used as inputs into the analysis
	# Find the mapping analyses that produced those tagaligns
	# Find the filtered bams from those analyses
	# Build the stage dict and return it
	logger.debug('in get_mapping_stages with peaks_analysis %s; experiment %s; reps %s' %(peaks_analysis['id'], experiment['accession'], reps))
	peaks_stages = peaks_analysis.get('stages')
	peaks_stage = next(stage for stage in peaks_stages if stage['execution']['name'] == "ENCODE Peaks")
	tas = [dxpy.describe(peaks_stage['execution']['input']['rep%s_ta' %(n)]) for n in reps]
	mapping_jobs = [dxpy.describe(ta['createdBy']['job']) for ta in tas]
	mapping_analyses = [dxpy.describe(mapping_job['analysis']) for mapping_job in mapping_jobs]
	mapping_stages = [analysis.get('stages') for analysis in mapping_analyses]
	#mapping_stages is a list of lists (one list per rep) of stages, each of which contains somewhere the Filter and QC stage for that rep
	#So we need to flatten that list of lists back to a single list with one stage per rep
	filter_qc_stages = [next(stage for stage in rep_mapping_stages if stage['execution']['name'].startswith("Filter and QC")) for rep_mapping_stages in mapping_stages]
	bams = [dxpy.describe(stage['execution']['output']['filtered_bam']) for stage in filter_qc_stages]
	fastqs = get_rep_fastqs(experiment, keypair, server, reps)

	bam_metadata = common.merge_dicts({
		'file_format': 'bam',
		'output_type': 'alignments'
		}, common_metadata)

	rep_mapping_stages = [
		{
			"_inputs": {
				'files': [
					{'name': 'rep%s_fastqs' %(n),	'derived_from': None,			'metadata': {}, 'encode_object': fastqs[n]}
				],
				'qc': []
			},
			"Filter and QC*" : {
				'files': [
					{'name': 'rep%s_bam' %(n),	'derived_from': 'rep%s_fastqs' %(n),	'metadata': bam_metadata}
				],
				'qc': [],
				'stage_metadata': {} #initialized below
			}
		} for n,r in enumerate(reps)]

	for n, stages in enumerate(rep_mapping_stages):
		for stage_name in stages:
			if not stage_name.startswith('_'):
				rep_mapping_stages[n][stage_name].update({'stage_metadata': get_stage_metadata(mapping_analyses[n], stage_name)})

	return rep_mapping_stages

def get_peak_stages(peaks_analysis, mapping_stages, experiment, keypair, server):

	logger.debug('in get_peak_stages with peaks_analysis %s; experiment %s' %(peaks_analysis['id'], experiment['accession']))

	#TODO this shoul actually get the bam accessioned (or found) by the accessioning step in this script
	rep1_bam, rep2_bam = get_rep_bams(experiment, common_metadata['assembly'], keypair, server)

	narrowpeak_metadata = common.merge_dicts({
		'file_format': 'bed',
		'file_format_type': 'narrowPeak',
		'file_format_specifications': ['ENCODE:narrowPeak.as'],
		'output_type': 'peaks'},
		common_metadata)

	replicated_narrowpeak_metadata = common.merge_dicts({
		'file_format': 'bed',
		'file_format_type': 'narrowPeak',
		'file_format_specifications': ['ENCODE:narrowPeak.as'],
		'output_type': 'replicated peaks'},
		common_metadata)

	narrowpeak_bb_metadata = common.merge_dicts({
		'file_format': 'bigBed',
		'file_format_type': 'narrowPeak',
		'file_format_specifications': ['ENCODE:narrowPeak.as'],
		'output_type': 'peaks'},
		common_metadata)

	replicated_narrowpeak_bb_metadata = common.merge_dicts({
		'file_format': 'bigBed',
		'file_format_type': 'narrowPeak',
		'file_format_specifications': ['ENCODE:narrowPeak.as'],
		'output_type': 'replicated peaks'},
		common_metadata)

	fc_signal_metadata = common.merge_dicts({
		'file_format': 'bigWig',
		'output_type': 'fold change over control'},
		common_metadata)

	pvalue_signal_metadata = common.merge_dicts({
		'file_format': 'bigWig',
		'output_type': 'signal p-value'},
		common_metadata)

	peak_stages = { #derived_from is by name here, will be patched into the file metadata after all files are accessioned
		"_inputs": {
			'files': [
				{'name': 'rep1_bam',				'derived_from': 'rep1_fastqs',			'metadata': {}, 'encode_object': rep1_bam},
				{'name': 'rep2_bam',				'derived_from': 'rep2_fastqs',			'metadata': {}, 'encode_object': rep2_bam}
			],
			'qc': []
		},
		"ENCODE Peaks" : {
			'files': [
				{'name': 'rep1_narrowpeaks',		'derived_from': ['rep1_bam'],				'metadata': narrowpeak_metadata},
				{'name': 'rep2_narrowpeaks',		'derived_from': ['rep2_bam'],				'metadata': narrowpeak_metadata},
				{'name': 'pooled_narrowpeaks',		'derived_from': ['rep1_bam', 'rep2_bam'],	'metadata': narrowpeak_metadata},
				{'name': 'rep1_narrowpeaks_bb',		'derived_from': ['rep1_narrowpeaks'],		'metadata': narrowpeak_bb_metadata},
				{'name': 'rep2_narrowpeaks_bb',		'derived_from': ['rep2_narrowpeaks'],		'metadata': narrowpeak_bb_metadata},
				{'name': 'pooled_narrowpeaks_bb',	'derived_from': ['pooled_narrowpeaks'],		'metadata': narrowpeak_bb_metadata},
				{'name': 'rep1_pvalue_signal',		'derived_from': ['rep1_bam'],				'metadata': pvalue_signal_metadata},
				{'name': 'rep2_pvalue_signal',		'derived_from': ['rep2_bam'],				'metadata': pvalue_signal_metadata},
				{'name': 'pooled_pvalue_signal',	'derived_from': ['rep1_bam', 'rep2_bam'],	'metadata': pvalue_signal_metadata},
				{'name': 'rep1_fc_signal',			'derived_from': ['rep1_bam'],				'metadata': fc_signal_metadata},
				{'name': 'rep2_fc_signal',			'derived_from': ['rep2_bam'],				'metadata': fc_signal_metadata},
				{'name': 'pooled_fc_signal',		'derived_from': ['rep1_bam', 'rep2_bam'],	'metadata': fc_signal_metadata}
			],
			'qc': [],
			'stage_metadata': {} # initialized below
		},
		"Overlap narrowpeaks": {
			'files': [
				{'name': 'overlapping_peaks',		'derived_from': ['rep1_narrowpeaks', 'rep2_narrowpeaks', 'pooled_narrowpeaks'], 'metadata': replicated_narrowpeak_metadata},
				{'name': 'overlapping_peaks_bb',	'derived_from': ['overlapping_peaks'], 'metadata': replicated_narrowpeak_bb_metadata}
			],
			'qc': ['npeaks_in', 'npeaks_out', 'npeaks_rejected'],
			'stage_metadata': {} # initialized below
		}
	}

	for stage_name in peak_stages:
		if not stage_name.startswith('_'):
			peak_stages[stage_name].update({'stage_metadata': get_stage_metadata(peaks_analysis, stage_name)})

	return peak_stages

def resolve_name_to_accession(stages, output_name):
	#given a dict of named stages, and the name of one of the stages' outputs, return that output's
	#ENCODE accession number
	logger.debug('resolve_name_to_accession %s' %(output_name))
	for stage_name in stages:
		for output in stages[stage_name]['files']:
			if output['name'] == output_name:
				encode_object = output.get('encode_object')
				if encode_object:
					return encode_object.get('accession')
				else:
					return None

def patch_file(payload, keypair, server, dryrun, force):
	logger.debug('in patch_file with %s' %(payload))
	accession = payload.pop('accession')
	url = urlparse.urljoin(server,'files/%s' %(accession))
	if dryrun:
		logger.info("Dry run.  Would PATCH: %s with %s" %(accession, payload))
		logger.info("Dry run.  Returning unchanged file object")
		new_file_object = common.encoded_get(urlparse.urljoin(server,'/files/%s' %(accession)), keypair)
	else:
		r = requests.patch(url, auth=keypair, headers={'content-type': 'application/json'}, data=json.dumps(payload))
		try:
			r.raise_for_status()
		except:
			logger.warning('PATCH file object failed: %s %s' % (r.status_code, r.reason))
			logger.warning(r.text)
			new_file_object = None
		else:
			new_file_object = r.json()['@graph'][0]
			logger.info("Patched: %s" %(new_file_object.get('accession')))
	
	return new_file_object

def accession_file(f, keypair, server, dryrun, force):
	#check for duplication
	#download
	#calculate md5 and add to f.md5sum
	#post file and get accession, upload credentials
	#upload to S3
	#remove the local file (to save space)
	#return the ENCODEd file object
	logger.debug('in accession_file with f %s' %(f))
	dx = f.pop('dx')
	for tag in dx.tags:
		m = re.search(r'(ENCFF\d{3}\D{3})|(TSTFF\D{6})', tag)
		if m:
			logger.info('%s appears to contain ENCODE accession number in tag %s ... skipping' %(dx.get_id(),m.group(0)))
			if not force:
				return
	url = urlparse.urljoin(server, 'search/?type=file&submitted_file_name=%s&format=json&frame=object&limit=all' %(f.get('submitted_file_name')))
	r = requests.get(url,auth=keypair)
	try:
		r.raise_for_status()
	except:
		logger.warning('Duplicate accession check failed: %s %s' % (r.status_code, r.reason))
		logger.debug(r.text)
		duplicate_found = False
	else:
		if r.json()['@graph']:
			for duplicate_item in r.json()['@graph']:
				if duplicate_item.get('status')  in ['deleted', 'replaced', 'revoked']:
					logger.info("A potential duplicate file was found but its status=%s ... proceeding" %(duplicate_item.get('status')))
					duplicate_found = False
				else:
					logger.info("Found potential duplicate: %s" %(duplicate_item.get('accession')))
					local_file_size = dx.describe()['size']
					if local_file_size ==  duplicate_item.get('file_size'):
						logger.info("%s %s: File sizes match, assuming duplicate." %(str(local_file_size), duplicate_item.get('file_size')))
						duplicate_found = True
						break
					else:
						logger.info("%s %s: File sizes differ, assuming new file." %(str(local_file_size), duplicate_item.get('file_size')))
						duplicate_found = False
		else:
			duplicate_found = False

	if duplicate_found:
		if force:
			logger.info("Duplicate detected, but force=true, so continuing")
		else:
			logger.info("Duplicate detected, skipping")
			return duplicate_item

	local_fname = dx.name
	logger.info("Downloading %s" %(local_fname))
	dxpy.download_dxfile(dx.get_id(),local_fname)
	f.update({'md5sum': common.md5(local_fname)})
	f['notes'] = json.dumps(f.get('notes'))

	url = urlparse.urljoin(server,'files/')
	if dryrun:
		logger.info("Dry run.  Would POST %s" %(f))
		new_file_object = {}
	else:
		r = requests.post(url, auth=keypair, headers={'content-type': 'application/json'}, data=json.dumps(f))
		try:
			r.raise_for_status()
			new_file_object = r.json()['@graph'][0]
			logger.info("New accession: %s" %(new_file_object.get('accession')))
		except:
			logger.warning('POST file object failed: %s %s' % (r.status_code, r.reason))
			logger.warning(r.text)
			new_file_object = {}
			if r.status_code == 409:
				try: #cautiously add a tag with the existing accession number
					if calculated_md5 in r.json().get('detail'):
						url = urlparse.urljoin(server,'/search/?type=file&md5sum=%s' %(calculated_md5))
						r = requests.get(url,auth=keypair)
						r.raise_for_status()
						accessioned_file = r.json()['@graph'][0]
						existing_accession = accessioned_file['accession']
						dx.add_tags([existing_accession])
						logger.info('Already accessioned.  Added %s to dxfile tags' %(existing_accession))
				except:
					logger.info('Conflict does not appear to be md5 ... continuing')
		if new_file_object:
			creds = new_file_object['upload_credentials']
			env = os.environ.copy()
			env.update({
				'AWS_ACCESS_KEY_ID': creds['access_key'],
				'AWS_SECRET_ACCESS_KEY': creds['secret_key'],
				'AWS_SECURITY_TOKEN': creds['session_token'],
			})

			logger.info("Uploading file.")
			start = time.time()
			try:
				subprocess.check_call(['aws', 's3', 'cp', local_fname, creds['upload_url'], '--quiet'], env=env)
			except subprocess.CalledProcessError as e:
				# The aws command returns a non-zero exit code on error.
				logger.error("Upload failed with exit code %d" % e.returncode)
				upload_returncode = e.returncode
			else:
				upload_returncode = 0
				end = time.time()
				duration = end - start
				logger.info("Uploaded in %.2f seconds" % duration)
				dx.add_tags([new_file_object.get('accession')])
		else:
			upload_returncode = -1

	try:
		os.remove(local_fname)
	except:
		pass

	if dryrun:
		return
	else:
		# logger.debug('Getting new file object for %s', %(new_file_object.get('accession')))
		# return common.encoded_get(urlparse.urljoin(server,'/files/%s' %(new_file_object.get('accession')), keypair))
		return new_file_object

def accession_analysis_step_run(analysis_step_run_metadata, keypair, server, dryrun, force):
	url = urlparse.urljoin(server,'/analysis-step-runs/')
	if dryrun:
		logger.info("Dry run.  Would POST %s" %(analysis_step_run_metadata))
		new_object = {}
	else:
		r = requests.post(url, auth=keypair, headers={'content-type': 'application/json'}, data=json.dumps(analysis_step_run_metadata))
		try:
			r.raise_for_status()
		except:
			logger.warning('POST analysis_step_run object failed: %s %s' % (r.status_code, r.reason))
			logger.warning(r.text)
			if r.status_code == 409:
				url = urlparse.urljoin(server,"/%s" %(analysis_step_run_metadata['aliases'][0])) #assumes there's only one alias
				new_object = common.encoded_get(url, keypair)
				logger.info('Using existing analysis_step_run object %s' %(new_object.get('@id')))
			else:
				new_object = {}
		else:
			new_object = r.json()['@graph'][0]
			logger.info("New analysis_step_run uuid: %s" %(new_object.get('uuid')))
	return new_object

def accession_analysis_files(peaks_analysis_id, keypair, server, dryrun, force):
	peaks_analysis_id = peaks_analysis_id.strip()
	peaks_analysis = dxpy.describe(peaks_analysis_id)
	project = peaks_analysis.get('project')

	m = re.match('^(ENCSR[0-9]{3}[A-Z]{3}) Peaks',peaks_analysis['executableName'])
	if m:
		experiment_accession = m.group(1)
		logger.info(experiment_accession)
	else:
		logger.info("No accession in %s, skipping." %(peaks_analysis['executableName']))
		return

	experiment = common.encoded_get(urlparse.urljoin(server,'/experiments/%s' %(experiment_accession)), keypair)

	mapping_stages = get_mapping_stages(peaks_analysis, experiment, keypair, server)

	peak_stages = get_peak_stages(peaks_analysis, mapping_stages, experiment, keypair, server)

	#accession all the output files, except don't accession what's in _input (_*)
	for (stage_name, outputs) in peak_stages.iteritems():
		if not stage_name.startswith('_'):
			stage_metadata = outputs['stage_metadata']
			for i,file_metadata in enumerate(outputs['files']):
				dx = dxpy.DXFile(stage_metadata['output'][file_metadata['name']], project=project)
				dx_desc = dx.describe()
				post_metadata = {
					'dx': dx,
					'notes': {
						'dx-id': dx.get_id(),
						'dx-createdBy': dx_desc.get('createdBy'),
						'qc': dict(zip(outputs['qc'],[stage_metadata['output'][metric] for metric in outputs['qc']]))}, #'aliases': ['ENCODE:%s-%s' %(experiment.get('accession'), static_metadata.pop('name'))],
					'dataset': experiment.get('accession'),
					'file_size': dx_desc.get('size'),
					'submitted_file_name': dx.get_proj_id() + ':' + '/'.join([dx.folder,dx.name])}
				post_metadata.update(file_metadata['metadata'])
				peak_stages[stage_name]['files'][i].update({'encode_object': accession_file(post_metadata, keypair, server, dryrun, force)})

				logger.debug('peak_stages:')
				logger.debug('%s' %(json.dumps(peak_stages[stage_name]['files'][i], sort_keys=True, indent=4, separators=(',', ': '))))


	#now that we have file accessions, loop again and patch derived_from, except don't patch what's in _input
	files = []
	for (stage_name, outputs) in peak_stages.iteritems():
		if not stage_name.startswith('_'):
			for file_metadata in outputs['files']:
				if file_metadata.get('encode_object'):
					accession = file_metadata['encode_object'].get('accession')
					patch_metadata = {
						'accession': accession,
						'derived_from':
							[resolve_name_to_accession(peak_stages, derived_from_name) for derived_from_name in file_metadata['derived_from']]
					}
					patched_file = patch_file(patch_metadata, keypair, server, dryrun, force)
					if patched_file:
						file_metadata['encode_object'] = patched_file
						files.append(patched_file)
					else:
						logger.error("%s PATCH failed ... skipping" %(accession))
				else:
					logger.warning('%s,%s: No encode object found ... skipping' %(stage_name, file_metadata['name']))
					continue

	analysis_step_versions = {
		'bwa-indexing-step-v-1' : [
			{
				'stage' : "",
				'file_names' : [],
				'status' : 'finished'
			}
		],
		'histone-bwa-alignment-step-v-1' : [
			{
				'stage' : mapping_stages[0]['Filter and QC*'],
				'file_names' : ['rep1_bam'],
				'status' : 'finished'
			},
			{
				'stage' : mapping_stages[1]['Filter and QC*'],
				'file_names' : ['rep2_bam'],
				'status' : 'finished'
			}
		],
		'histone-spp-peak-calling-step-v-1' : [
			{
				'stage' : peak_stages['ENCODE Peaks'],
				'file_names' : ['rep1_fc_signal', 'rep2_fc_signal', 'pooled_fc_signal', 'rep1_pvalue_signal', 'rep2_pvalue_signal', 'pooled_pvalue_signal', 'rep1_narrowpeaks', 'rep2_narrowpeaks', 'pooled_narrowpeaks'],
				'status' : 'finished'
			}
		],
		'histone-overlap-peaks-step-v-1' : [
			{
				'stage' : peak_stages['Overlap narrowpeaks'],
				'file_names' : ['overlapping_peaks'],
				'status' : 'finished'
			}
		],
		'histone-spp-peaks-to-bigbed-step-v-1' : [
			{
				'stage' : peak_stages['ENCODE Peaks'],
				'file_names' : ['rep1_narrowpeaks_bb', 'rep2_narrowpeaks_bb', 'pooled_narrowpeaks_bb'],
				'status' : 'virtual'
			}
		],
		'histone-spp-replicated-peaks-to-bigbed-step-v-1' : [
			{
				'stage' : peak_stages['Overlap narrowpeaks'],
				'file_names' : ['overlapping_peaks_bb'],
				'status' : 'virtual'
			}
		]
	}

	for (analysis_step_version_name, steps) in analysis_step_versions.iteritems():
		for step in steps:
			if not (step['stage'] and step['file_names']):
				logger.warning('%s missing stage metadata (files or stage_name) ... skipping' %(analysis_step_version_name))
				continue
			jobid = step['stage']['stage_metadata']['id']
			analysis_step_version = 'versionof:%s' %(analysis_step_version_name)
			alias = 'dnanexus:%s' %(jobid)
			if step.get('status') == 'virtual':
				alias += '-virtual-file-conversion-step'
			analysis_step_run_metadata = {
				'aliases': [alias],
				'analysis_step_version': analysis_step_version,
				'status': step['status'],
				'dx_applet_details': [{
					'dx_status': 'finished',
					'dx_job_id': 'dnanexus:%s' %(jobid),
				}]
			}
			analysis_step_run = accession_analysis_step_run(analysis_step_run_metadata, keypair, server, dryrun, force)
			for file_name in step['file_names']:
				file_accession = resolve_name_to_accession(peak_stages, file_name)
				patch_metadata = {
					'accession': file_accession,
					'step_run': analysis_step_run.get('@id')
				}
				patched_file = patch_file(patch_metadata, keypair, server, dryrun, force)

	return files


@dxpy.entry_point('main')
def main(outfn, assembly, debug, key, keyfile, dryrun, force, pipeline, analysis_ids=None, infile=None, project=None):

	if debug:
		logger.info('setting logger level to logging.DEBUG')
		logger.setLevel(logging.DEBUG)
	else:
		logger.info('setting logger level to logging.INFO')
		logger.setLevel(logging.INFO)

	if infile is not None:
		infile = dxpy.DXFile(infile)
		dxpy.download_dxfile(infile.get_id(), "infile")
		ids = open("infile",'r')
	elif analysis_ids is not None:
		ids = analysis_ids
	else:
		logger.error("Must supply one of --infile or a list of one or more analysis-ids")
		return

	authid, authpw, server = common.processkey(key, keyfile)
	keypair = (authid,authpw)

	common_metadata.update({'assembly': assembly})

	for (i, analysis_id) in enumerate(ids):
		logger.debug('debug %s' %(analysis_id))
		accessioned_files = accession_analysis_files(analysis_id, keypair, server, dryrun, force)

	logger.info("Accessioned: %s" %([f.get('accession') for f in accessioned_files]))

	common.touch(outfn)
	outfile = dxpy.upload_local_file(outfn)

	output = {}
	output["outfile"] = dxpy.dxlink(outfile)

	return output

dxpy.run()
